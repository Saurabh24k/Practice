{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e10be3f-538e-4ca5-b087-3169e2620fce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "753e2a19-bf7c-4689-a435-6d03a965ecee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', ',', '', ' ', 'is', ' ', 'some', ' ', 'text', ' ', 'that', ' ', \"does'nt\", ' ', 'mean', ' ', 'any', ' ', 'thing', '.', '']\n"
     ]
    }
   ],
   "source": [
    "text = \"This, is some text that does'nt mean any thing.\"\n",
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0be8ca25-8ba4-442d-a186-3b54d2ae1fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', ',', 'is', 'some', 'text', 'that', \"does'nt\", 'mean', 'any', 'thing', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "af272923-6298-4459-8474-ffb002ea047f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters available : 375476\n",
      "A Brief History of Time - Stephen Hawking\n",
      "Chapter 1 - Our Picture of the Universe\n",
      "Chapter 2 - Space\n"
     ]
    }
   ],
   "source": [
    "with open(\"bht.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total characters available :\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "752225df-cf6e-470a-8e55-635a3353ce09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens available : 73010\n",
      "['A', 'Brief', 'History', 'of', 'Time', '-', 'Stephen', 'Hawking', 'Chapter', '1', '-', 'Our', 'Picture', 'of', 'the', 'Universe', 'Chapter', '2', '-', 'Space', 'and', 'Time', 'Chapter', '3', '-', 'The', 'Expanding', 'Universe', 'Chapter', '4', '-', 'The', 'Uncertainty', 'Principle', 'Chapter', '5', '-', 'Elementary', 'Particles', 'and', 'the', 'Forces', 'of', 'Nature', 'Chapter', '6', '-', 'Black', 'Holes', 'Chapter', '7', '-', 'Black', 'Holes', 'Ain', \"'\", 't', 'So', 'Black', 'Chapter', '8', '-', 'The', 'Origin', 'and', 'Fate', 'of', 'the', 'Universe', 'Chapter', '9', '-', 'The', 'Arrow', 'of', 'Time', 'Chapter', '10', '-', 'Wormholes', 'and', 'Time', 'Travel', 'Chapter', '11', '-', 'The', 'Unification', 'of', 'Physics', 'Chapter', '12', '-', 'Conclusion', 'Glossary', 'Acknowledgments', '&', 'About', 'The']\n"
     ]
    }
   ],
   "source": [
    "raw_data = re.split(r'(\\s|[.,!?;:\"\\'()\\[\\]{}<>|\\\\/`~@#$%^&*_+=-]|\\.\\.\\.)', raw_text)\n",
    "'''\n",
    "Removal of white spaces is crucial aspect to consider if the data \n",
    "is like codes or where the spaces matter then include the spaces \n",
    "in the data otherwise its better to not use them.\n",
    "'''\n",
    "\n",
    "raw_data = [token for token in raw_data if token.strip()]\n",
    "print(\"Total tokens available :\",len(raw_data))\n",
    "print(raw_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "576a714c-3a54-4c93-ac44-bafb201c1fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5088\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(raw_data)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "print(len(all_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "870ade41-0de7-4c9b-8d48-9c73d926525c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "('&', 2)\n",
      "(\"'\", 3)\n",
      "('(', 4)\n",
      "(')', 5)\n",
      "('+', 6)\n",
      "(',', 7)\n",
      "('-', 8)\n",
      "('.', 9)\n",
      "('/', 10)\n",
      "('0', 11)\n",
      "('00', 12)\n",
      "('000', 13)\n",
      "('000000003335640952', 14)\n",
      "('004th', 15)\n",
      "('1', 16)\n",
      "('10', 17)\n",
      "('100', 18)\n",
      "('11', 19)\n",
      "('12', 20)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "for i, token in enumerate(vocab.items()):\n",
    "    print(token)\n",
    "    if i>= 20:\n",
    "        break\n",
    "        \n",
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d6983caf-358e-47bb-82ab-7966c313eec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_int = vocab\n",
    "        self.int_str = {i:s for s,i in vocab.items()}\n",
    "        \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'(\\s|[.,!?;:\"\\'()\\[\\]{}<>|\\\\/`~@#$%^&*_+=-]|\\.\\.\\.)', text)\n",
    "        \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([.,!?;:\"\\'()\\[\\]{}<>|\\\\/`~@#$%^&*_+=-]|\\.\\.\\.)', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e36e8fdc-64a7-447c-b5e1-b740f3f6e04d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[723, 4596, 3428, 4249, 7, 1537, 2433, 4596, 3300, 2832, 2035, 2703, 1419, 4596, 3214, 3453, 2505, 2839, 4596, 1879, 3248, 1385, 3860, 1537, 252, 4, 4596, 312, 208, 389, 4136, 5, 1207, 3508, 1683, 9]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"On the observational side, by far the most important development has been the measurement of fluctuations in\n",
    "the cosmic microwave background radiation by COBE (the Cosmic Background Explorer satellite) and other\n",
    "collaborations.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "deaf0a5a-12c9-49f5-9349-a7f798e9ffa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On the observational side, by far the most important development has been the measurement of fluctuations in the cosmic microwave background radiation by COBE( the Cosmic Background Explorer satellite) and other collaborations.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dede352a-8d30-4902-b54c-822f151144ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_int = vocab\n",
    "        self.int_str = {i:s for s,i in vocab.items()}\n",
    "        \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'(\\s|[.,!?;:\"\\'()\\[\\]{}<>|\\\\/`~@#$%^&*_+=-]|\\.\\.\\.)', text)\n",
    "        \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([.,!?;:\"\\'()\\[\\]{}<>|\\\\/`~@#$%^&*_+=-]|\\.\\.\\.)', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6191c7d6-2067-4511-ba6b-077bda5b72c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea?<endoftext|>Hello, would you like some Coffee?\n",
      "[5087, 7, 2133, 5003, 3084, 5087, 130, 5087, 5087, 5087, 5087, 5087, 7, 4986, 5003, 3084, 4325, 5087, 130]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"Hello, would you like some Coffee?\"\n",
    "\n",
    "text = \"<endoftext|>\".join((text1, text2))\n",
    "\n",
    "print(text)\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1c63e307-1c25-47bb-9a7d-a76c8810f9f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like<|unk|>?<|unk|><|unk|><|unk|><|unk|><|unk|>, would you like some<|unk|>?'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70238339-abfb-4903-8178-1d3bbee0aafd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
